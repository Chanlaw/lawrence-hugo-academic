---
title: Lawrence Chan
role: PhD Candidate
avatar_filename: avatar.png
bio: I do AI Alignment research.
interests:
  - AI alignment
  - mechanistic interpretability
  - scalable oversight
social:
  - icon: envelope
    icon_pack: fas
    link: mailto:lawrence.chan@live.cn
  - icon: twitter
    icon_pack: fab
    link: https://twitter.com/justanotherlaw
    label: Follow me on Twitter
    display:
      header: true
  - icon: graduation-cap
    icon_pack: fas
    link: https://scholar.google.com/citations?user=6Id1G8cAAAAJ
  - icon: github
    icon_pack: fab
    link: https://github.com/Chanlaw
organizations:
  - name: Berkeley AI Research
    url: https://bair.berkeley.edu/
education:
  courses:
    - course: BAS in Computer Science, Logic; BS in Economics
      institution: University of Pennsylvania
      year: 2018
superuser: true
status:
  icon:
last_name: Chan
highlight_name: true
first_name: Lawrence
email: ""
---

As of January 2023, I'm currently working at the [Alignment Research Center](https://alignment.org/) doing evaluations of large language models. Previously, I was at Redwood Research, where I worked on [adversarial training](https://arxiv.org/abs/2205.01663) and [neural network interpretability](https://www.alignmentforum.org/posts/JvZhhzycHu2Yd57RN/causal-scrubbing-a-method-for-rigorously-testing).
{style="text-align: left;"}

In early 2022, I went on leave from my PhD at UC Berkeley, where I was advised by [Anca Dragan](http://people.eecs.berkeley.edu/~anca/) and [Stuart Russell](http://people.eecs.berkeley.edu/~russell/), to work on more hands-on AI alignment projects.
{style="text-align: left;"}

My main research interests are [mechanistic interpretability](https://transformer-circuits.pub/2022/mech-interp-essay/index.html) and [scalable oversight](https://arxiv.org/abs/2211.03540). In the past, I've also done conceptual work on learning human values.
{style="text-align: left;"}

<!-- Previously,
{style="text-align: left;"} -->
